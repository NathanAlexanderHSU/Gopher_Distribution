###Post driving survey Maxent

#general data manipulation packages
install.packages("spatstat")
install.packages("rgdal")
install.packages("raster")
install.packages("maptools")
install.packages("rgeos")
install.packages("rJava")
install.packages("sp")
install.packages("PresenceAbsence")
install.packages('devtools')

#specific packages for extracting coordinates from photos
install.packages('exifr')
install.packages('dplyr')
install.packages('leaflet')
install.packages('tidyverse')

#MaxEnt package and elevation layer
install.packages('dismo')

#PRISM package for prism data
install.packages('prism',dependencies=TRUE)

#Package to thin spatial points
install.packages('spThin')

#dependencies that failed to load 
install.packages('stringi')
install.packages('sf')

################################
require(spThin)
require(spatstat)
require(rgdal)
require(raster)
require(maptools)
require(rgeos)
require(dismo)
require(rJava)
require(sp)
require(PresenceAbsence)
library(exifr)
library(dplyr)
library(leaflet)
require(devtools)
require(prism)
require(stringi)
require(prism)
require(sf)

###Download MaxEnt here: https://biodiversityinformatics.amnh.org/open_source/maxent/
###move the 'maxent.jar' file from the download into the dismo package 'Java' folder

##############################################
##############################################
##############################################

### First, get the extent and map you want to limit the study to
##initial gopher shape files
##This will be the area which we will crop and mask the predictor layers to

illinois<-readOGR(dsn="C:/Users/Nathan/Desktop/Gopher/Presence_Only_shapefiles",layer="IL_BNDY_County_Py")

#ensure that it read correctly
plot(illinois)

############################################
############################################
###Next, we want to read in our predictor layers
###Predictor Variables


############################################
##elevation-this is from the dismo package 

elev <- getData('alt', download=TRUE, country='USA', mask=TRUE)
elev.48 <- elev[[1]]
crs(elev.48)

############################################
##Prism---we can read in the Raster Stack that we've already created


#This following is to read in PRISM data from the prism package. You can select the type of data and the years you want data from

#ann_precip<-get_prism_annual(type='ppt', years=2008:2018)
#list_prism<-ls_prism_data(absPath = FALSE, name = FALSE)
#prism_2008_18<-prism_stack(list_prism)

#have to fix crs (switch to WGS 84) for illinois to mask

#prism_84 <- projectRaster(prism_2008_18,elev.48)

#read in the raster stack we created

#becuase I've already processed the PRISM data, we can read in directly the stack
prism_84<-stack("C:\\Users\\Nathan\\Desktop\\Gopher\\Presence_Only_shapefiles\\PRISM_Raster\\Prism_84.tif")

##crop the raster stack to be just Illinois
cr.clim<-crop(prism_84,illinois)
cr.clim<-mask(cr.clim,illinois)

##calculate the standard deviation of precip
sd_precip<-calc(cr.clim,sd)
names(sd_precip)<-"Precipitation SD"

##calculate the mean precip
avg_precip<-calc(cr.clim, mean)
names(avg_precip)<-"Precipitation Mean"

##crop elevation to illinois
cr.elev<-crop(elev.48,illinois)
cr.elev<-mask(cr.elev,illinois)
names(cr.elev)<-"Elevation"

######################################
##soil-from SSURGO dataset

soil<-readOGR(dsn="C:/Users/Nathan/Desktop/Gopher/Environmental Shapefiles/Soil_trial",layer="SSURGO_IL_Text1")

##Use texture, drainage, and sand percentage as predictors

soil_rast_texture<-rasterize(soil,cr.elev,'texdesc')
names(soil_rast_texture)<-"Soil Texture"
soil_rast_drainage<-as.factor(rasterize(soil,cr.elev,'drainagecl'))
names(soil_rast_drainage)<-"Soil Drainage"
soil_rast_sand<-rasterize(soil,cr.elev,'sandtotal_')
names(soil_rast_sand)<-"Soil Sand Percentage"


###################################################
##read in landcover type-cropland data layer from USDA NASS

land_use<-raster("C:/Users/Nathan/Desktop/Gopher/Environmental Shapefiles/c2016_30m_cdls_84.img")

land_use_il<-crop(land_use,illinois)

##resample to the 800m resolution of elevation

#use 'ngb' for nearest neighbor. Must use this for categorical values

land_use_il_re<-resample(land_use_il,cr.elev,method='ngb')

land_use_il_re<-mask(land_use_il_re,illinois)

names(land_use_il_re)<-"Land Cover"

cr.precip_sd<-resample(sd_precip,cr.elev)
cr.precip_avg<-resample(avg_precip,cr.elev)

##convert elevation to slope
slope<-terrain(cr.elev,opt='slope',unit='degrees',neighbors=8)
names(slope)<-"Slope"

predictors <- stack(cr.precip_sd,cr.precip_avg,cr.elev,slope,soil_rast_texture,soil_rast_drainage,soil_rast_sand,land_use_il_re)

plot(predictors)

#Ensure that no predictors are highly correlated
pairs(predictors)

###############################################
###############################################
###############################################

#setwd(choose.dir())
####Occupied Sites

##This project started out as a presence-absence study, but due to limited presences, we switched to a presence-pseudoabsence framework
#below is reading in all the presences we recorded

##############################################
#Pilot survey presences
#pilot<-readOGR(dsn="C:/Users/Nathan/Desktop/Gopher/Presence_Only_shapefiles",layer="Pilot_occuppied")
#pilot_sp<-SpatialPoints(pilot)
#pilot_sp

##############################################
#Presence-absence survey presences
#PA<-readOGR(dsn="C:/Users/Nathan/Desktop/Gopher/Presence_Only_shapefiles",layer="PA_Survey_occuppied")
#PA_sp<-SpatialPoints(PA)
#PA_sp

#############################################
#Bluet Points-a retired IDNR person sent us geo-referenced photos. This code is used to extract coordinates
#library(exifr)
#library(dplyr)
#library(leaflet)
#setwd("C:\\Users\\Nathan\\Desktop\\Gopher\\Bluett Photos")
#files<-list.files()
#files
#dat <- exifr::read_exif(files)

#Bluett_coords<-cbind(dat$FileName,dat$GPSLongitude,dat$GPSLatitude)

#remove the ones without a geotag
#Bluett_coords<-rbind(Bluett_coords[1:33,],Bluett_coords[40:50,],Bluett_coords[52:75,])

#B_pres<-cbind(as.vector(Bluett_coords[,2]),as.vector(Bluett_coords[,3]))

###need to truncate the coordinates from the Bluett Photos

#presence_only_x<-round(as.numeric(B_pres[,1]), digits = 6)
#presence_only_y<-round(as.numeric(B_pres[,2]), digits = 6)
#Bluet_df<-cbind(presence_only_x,presence_only_y)

#make into a spatial points data frame
#Bluet_sp<-SpatialPoints(Bluet_df)
#Bluet_sp

############################################
##we can now download all presence points available via the Global Biodiversity Information facility

#sp.occur <- gbif(genus='Geomys', species='bursarius', geo=TRUE,removeZeros=TRUE,ext=illinois)

#restrict the occurrences to be within the past 10 years

#sp.occur_2008<-sp.occur[which(sp.occur$year>=2008),]

#gbif_sp<-SpatialPoints(cbind(sp.occur_2008$lon,sp.occur_2008$lat))


############################################
##Driving Survey-due to the low number of presences from the presence-absence survey, we conducted driving surveys in
## 2018 to locate gopher mounds

#drive<-read.csv("C:\\Users\\Nathan\\Desktop\\Gopher\\Presence_Only_shapefiles\\Driving_survey.csv")
#drive_lon<-drive[1:74,4]
#drive_lat<-drive[1:74,3]
#drive_lon<-round(as.numeric(as.character(drive[1:74,4])), digits = 6)
#drive_lat<-round(as.numeric(as.character(drive[1:74,3])), digits = 6)
#drive_sp<-SpatialPoints(cbind(drive_lon,drive_lat))
#drive_sp

##############################################
###merge all the spatial presences of post 2008- this will create one file of presencee points

#presence_only<-SpatialPoints(rbind(drive_sp,gbif_sp,Bluet_sp,pilot_sp,PA_sp))
#total.pts<-presence_only

#total.pts<-SpatialPointsDataFrame(total.pts,data=data.frame(coordinates(total.pts)))

#write the total presence points to a file so we can read directly in for future analyses
#writeOGR(obj=total.pts, dsn="C:\\Users\\Nathan\\Desktop\\Gopher\\Presence_Only_shapefiles", layer="TotalPts_Aug2019", driver="ESRI Shapefile") # this is in geographical projection

###############################################
#Read in aggregated data source
total.pts<-readOGR("C:\\Users\\Nathan\\Desktop\\Gopher\\Presence_Only_shapefiles", layer="TotalPts_Aug2019")

plot(illinois)
points(total.pts)

####### Read in original raster if you want to clip to it
#original_raster<-raster("C:\\Users\\Nathan\\Desktop\\Gopher\\Survey Design\\Min_Thresh_stand.asc")

#############################################
############################################
############################################

####MAXENT CODE######

#make sure that your predictors are accurate
names(predictors)
reduced.preds <- predictors

################################
#make sure the crs is all the same. All of these files were WGS 84, so did not 
#reproject them. If they are in different coordinate systems, use a project function

crs(illinois)<-crs(predictors)
crs(total.pts)<-crs(predictors)

##################################
##################################
##Occurence point data manipulation
###make sure that you're only looking at occurrences in illinois. Sometimes GBIF pulls a few from Missouri
total.pts <- gIntersection(total.pts, illinois, byid=TRUE)
#create a data frame of the coordinates
pts_df<-coordinates(total.pts)

######################################
#Now thin the coordinates to a selected number of bins and a reasonable distance
#here, I thinned the data at 1 km 5 times
thinned_pts<-thin.algorithm(pts_df,thin.par=1,rep=5)

#visualize the thinned data-I'm just using the first thinning run

thinned_pts[[1]]
plot(illinois)
points(thinned_pts[[1]],pch=17)
points(thinned_pts[[2]],pch=17,col='red')
length(thinned_pts[[1]])
#set a seed to do a random selection of 75% of the data
set.seed(1)

########################################
##Next we will create a training and test dataset

#create a vector of all the pts from the first thinned run
for( i in 1:5)
pt_total<-as.vector(1:length(thinned_pts[[1]]))

#We can now subsample to create a 75% training dataset and a 25% testing dataset
pt_train_1<-sample(pt_total,3*length(pt_total)/4,replace=FALSE)
pt_test<-pt_total[-pt_train_1]

#Select the occupied points that are the training data
occ.pts<-total.pts[pt_train_1,]

#Select the occupied points that are the testing data (validation data)
val.pts<-total.pts[pt_test,]

#Visualize the output
plot(illinois)
points(occ.pts,col='red')
points(val.pts)

##############################################
##############################################
##Model creation
# Create a vector of all the beta paremeters. Beta parameters are a smoothing option for MaxEnt to prevent overfitting
beta.parameters <- c(1,2)

#Make sure that you will run the correct beta parameters

for(i in beta.parameters){
  print(paste("Run Maxent with", i, "as the beta parameter"))
}

##Now, we will create the candidate models. We will analyze all combination of parameters with all beta parameters
all.combos <- list()
pred.num <- nlayers(predictors)
#There must be 2 layers in each model at least for maxent to run

for(i in 2:pred.num){
  all.combos <- c(all.combos, combn(1:pred.num, i, simplify=FALSE))
}

#ensure that all the models will be run 
all.combos

#Run a trial to ensure that everything is working correctly before the for-loop
#Label your categorical predictors as factors
#We will have a smoothing parameter of 1 (none) and will do a jackknife, print the plots, and have 10 replicates of cross validation
#Remove Duplicates will remove occurrences that fall in the same grid cell. this means that your resolution will inform your data if
#it is not thinned beforehand or if the resolution is coarser than your thinning

neg.maxent <- maxent(predictors, factors=c("Soil.Texture","Land.Cover"), data.frame(occ.pts)[,1:2],removeDuplicates=TRUE, args=c("betamultiplier=1", "-J", "-P", "replicates=10"),
				path="C:\\Users\\Nathan\\Desktop\\Gopher\\Presence_Only_shapefiles\\Trials")

#########################################
#Once you make sure that everything is working well, you can then do a for-loop
#note that in the code, we are not doing jackknife, plots, nor replicates per run. This will make things faster

# Create a loop for each beta parameter
for(i in beta.parameters){ 
#create another for-loop for each combination of parameters
  for(j in 1:length(all.combos)){
#create the directory where each run will be stored. This labels the folders based on the beta parameter
#followed by the included predictor variables
output.dir <- paste("C:\\Users\\Nathan\\Desktop\\Gopher\\Presence_Only_shapefiles\\MaxEnt_2019_thin1_T2\\b", i, ".", 
                        paste(all.combos[[j]], collapse=''), sep="")   
print(output.dir)

#create if statements for all the categorical variables. If they are included in the c
#combindation of predictors, they will be assigned as a factor in the model

if(5 %in% all.combos[[j]]){
	if(6 %in% all.combos[[j]]){
		if(8 %in% all.combos[[j]]){ 
			model <- maxent(predictors[[all.combos[[j]]]],factors=c("Soil.Texture","Soil.Drainage","Land.Cover"), data.frame(occ.pts),
                    	args=c(paste("betamultiplier=", i, sep=""), "outputformat=raw"),
                        path=output.dir)
	
			}
	if(8 %in% all.combos[[j]]){ 
		model <- maxent(predictors[[all.combos[[j]]]],factors=c("Soil.Texture","Land.Cover"), data.frame(occ.pts),
                 	args=c(paste("betamultiplier=", i, sep=""), "outputformat=raw"),
                  path=output.dir)
	
		}
	}

if(6 %in% all.combos[[j]]){
		if(8 %in% all.combos[[j]]){ 
			model <- maxent(predictors[[all.combos[[j]]]],factors=c("Soil.Drainage","Land.Cover"), data.frame(occ.pts),
                    	args=c(paste("betamultiplier=", i, sep=""), "outputformat=raw"),
                        path=output.dir)
	
			}
	}
if(8 %in% all.combos[[j]]){ 
		model <- maxent(predictors[[all.combos[[j]]]],factors="Land.Cover", data.frame(occ.pts),
                 	args=c(paste("betamultiplier=", i, sep=""), "outputformat=raw"),
                  path=output.dir)
	
		}
	}

model <- maxent(predictors[[all.combos[[j]]]], data.frame(occ.pts),
             args=c(paste("betamultiplier=", i, sep=""), "outputformat=raw"),
             path=output.dir)

    max.results <- dismo::predict(model, predictors[[all.combos[[j]]]])
    writeRaster(max.results, paste(output.dir, "/maxent.asc", sep=""),
                format="ascii")

    print(paste(j, " of ", length(all.combos), ": ", Sys.time(), sep=""))
  }
  
}

########################################
########################################
########################################

#Model Selection- we will create output data for ENMTools to do model selection

# need the list of occurence data points with species ID
name <- rep("geomys",nrow(data.frame(occ.pts)))
#have to subset longitude and latitude column based on INHS or GBIF data
Long<-coordinates(occ.pts)[,1]

Lat<-coordinates(occ.pts)[,2]

#make sure that points are assigned correctly
plot(illinois)
points(Long,Lat,col="blue")

#compile and write the species 
enmtools.occpts <- data.frame(name, Long, Lat)
colnames(enmtools.occpts) <- c("Species", "X", "Y")
write.csv(enmtools.occpts,"C:\\Users\\Nathan\\Desktop\\Gopher\\Presence_Only_shapefiles\\MaxEnt_2019_thin1_T2\\geomys.csv" , row.names=FALSE)

######################################
#Now we need to read all the file locations for where we wrote the MaxEnt output

setwd("C:\\Users\\Nathan\\Desktop\\Gopher\\Presence_Only_shapefiles\\MaxEnt_2019_thin1_T2")

dirs <- list.files()

##Now we will create a csv that has file paths for the species occurrence file, and for each maxent asc file and each species lambda files

models <- NULL
for(i in 1:length(dirs)){
  models[i] <- paste("C:\\Users\\Nathan\\Desktop\\Gopher\\Presence_Only_shapefiles\\MaxEnt_2019_thin1_T2\\geomys.csv,",
				"C:\\Users\\Nathan\\Desktop\\Gopher\\Presence_Only_shapefiles\\MaxEnt_2019\\",dirs[i], 
			"\\maxent.asc,C:\\Users\\Nathan\\Desktop\\Gopher\\Presence_Only_shapefiles\\MaxEnt_2019_thin1_T2\\", dirs[i],
                    "\\species.lambdas", sep="")
}

write.csv(models, "C:\\Users\\Nathan\\Desktop\\Gopher\\Presence_Only_shapefiles\\MaxEnt_2019_thin1_T2\\maxent_models.csv", row.names=FALSE, quote=FALSE, col.names=FALSE)
#####################################
#For records, write your testing and training dataset
write.csv(occ.pts,"Train_pts.csv")
write.csv(val.pts,"Test_pts.csv")

####In ENMTools available here: http://enmtools.blogspot.com/
####do model selection on the file 'maxent_models.csv'
#### sort based on AICc with lowest at the top

#############################################
#############################################

#Next, create a stack of the predictors included in the top model to re-run. This time we will
#include jackknife, plots, and cross validation
#We will write this to a new folder called 'Selected_Model'

pred<-stack(predictors[[2:3]],predictors[[5:6]])

neg.maxent <- maxent(pred, data.frame(occ.pts)[,1:2],removeDuplicates=TRUE, args=c("betamultiplier=1", "-J", "-P","replicates=10"),
				path="C:\\Users\\Nathan\\Desktop\\Gopher\\Presence_Only_shapefiles\\Selected_Model")


#now we can create a map based on our SDM
InPred<-predict(neg.maxent,pred)
plot(InPred)
points(total.pts)
points(val.pts,pch=17,col='red')

#write the raster output
writeRaster(InPred,"C:\\Users\\Nathan\\Desktop\\Gopher\\Presence_Only_shapefiles\\Selected_Model\\geomys_75.asc",format="ascii",overwrite=TRUE)

##############################################
####Next, we can look at the output for running the top model with just the validation occurences
neg.maxent_val <- maxent(pred, data.frame(val.pts)[,1:2],removeDuplicates=TRUE, args=c("betamultiplier=1", "-J", "-P"),
				path="C:\\Users\\Nathan\\Desktop\\Gopher\\Presence_Only_shapefiles\\Selected_Model_validation")


###############################################
##Next, we can look at AUC curves and thresholds
set.seed(2)
#create the background points
bg <- randomPoints(InPred, 1000)

e1 <- evaluate(neg.maxent, p=val.pts, a=bg, x=pred)
plot(e1, 'ROC')
e2 <- evaluate(neg.maxent, p=occ.pts, a=bg, x=pred)
plot(e2,'ROC')
threshold(e1)
threshold(e2)

